`low_cpu_mem_usage` was None, now default to True since model is quantized.
Namespace(backend='Llama3.1-8B-Instruct', temperature=0.7, task='MATH2', task_start_index=0, task_end_index=100, naive_run=False, method_select='greedy', n_generate_sample=3, n_evaluate_sample=2, n_select_sample=2)
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:23,  7.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:18,  9.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:29<00:10, 10.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.08s/it]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
